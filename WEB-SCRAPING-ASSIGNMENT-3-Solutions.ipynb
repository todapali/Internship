{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6560debd",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment 3 : Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6dfb0b",
   "metadata": {},
   "source": [
    "# Q1. Write a python program which searches all the product under a particular product vertical from www.amazon.in. The product verticals to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c758d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Importing selenium webdriver \n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c79c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3556efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the homepage of Amazon.in\n",
    "driver.get('https://www.amazon.in/')\n",
    "# Asking the user to input the keywords he/she wants to search\n",
    "user_inp = input('Enter the product you want to search : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837939ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element_by_id(\"twotabsearchtextbox\")    # Locating searc_bar by id\n",
    "search_bar.clear()                                               # clearing search_bar\n",
    "search_bar.send_keys(user_inp)                                   # sending user input to search bar\n",
    "search_button = driver.find_element_by_xpath('//div[@class=\"nav-search-submit nav-sprite\"]/span/input')       # Locating search_button by xpath\n",
    "search_button.click()                                                                # Clicking the button to start search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bff038",
   "metadata": {},
   "source": [
    "# Q2. 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a dataframe and csv. In case if any product vertical has less than 3 pages in search results then scrape all the products available under that product vertical. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Rating\", \"No. of Ratings\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Other Details\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991de885",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page = 0\n",
    "end_page = 3\n",
    "urls = []\n",
    "for page in range(start_page,end_page+1):\n",
    "    try:\n",
    "        page_urls = driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-no-outline\"]')\n",
    "        \n",
    "        # appending all the urls on current page to urls list\n",
    "        for url in page_urls:\n",
    "            url = url.get_attribute('href')     # Scraping the url from webelement\n",
    "            if url[0:4]=='http':                # Checking if the scraped data is a valid url or not\n",
    "                urls.append(url)                # Appending the url to urls list\n",
    "        print(\"Product urls of page {} has been scraped.\".format(page+1))\n",
    "        \n",
    "        # Moving to next page\n",
    "        nxt_button = driver.find_element_by_xpath('//li[@class=\"a-last\"]/a')      # Locating the next_button which is active\n",
    "        if nxt_button.text == 'Next→':                                            # Checking if the button located is next button\n",
    "            nxt_button.click()                                                    # Clicking the next button\n",
    "            time.sleep(5)                                                         # time delay of 5 seconds\n",
    "        # If the current active button is not next button, the we will check if the next button is inactive or not    \n",
    "        elif driver.find_element_by_xpath('//li[@class=\"a-disabled a-last\"]/a').text == 'Next→':    \n",
    "            print(\"No new pages exist. Breaking the loop\")  # Printing message and breakinf loop if we have reached the last page\n",
    "            break\n",
    "            \n",
    "    except StaleElementReferenceException as e:             # Handling StaleElement Exception   \n",
    "        print(\"Stale Exception\")\n",
    "        next_page = nxt_button.get_attribute('href')        # Extracting the url of next page\n",
    "        driver.get(next_page)                               # ReLoading the next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_dict = {}\n",
    "prod_dict['Brand']=[]\n",
    "prod_dict['Name']=[]\n",
    "prod_dict['Rating']=[]\n",
    "prod_dict['No. of ratings']=[]\n",
    "prod_dict['Price']=[]\n",
    "prod_dict['Return/Exchange']=[]\n",
    "prod_dict['Expected Delivery']=[] \n",
    "prod_dict['Availability']=[]\n",
    "prod_dict['Other Details']=[]\n",
    "prod_dict['URL']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d17686",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls[:4]:\n",
    "    driver.get(url)                                                        # Loading the webpage by url\n",
    "    print(\"Scraping URL = \", url)\n",
    "    #time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('//a[@id=\"bylineInfo\"]')      # Extracting Brand from xpath\n",
    "        prod_dict['Brand'].append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Brand'].append('-')\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//h1[@id=\"title\"]/span')      # Extracting Name from xpath\n",
    "        prod_dict['Name'].append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Name'].append('-')\n",
    "    \n",
    "    try:\n",
    "        rating = driver.find_element_by_xpath('//span[@id=\"acrPopover\"]')  # Extracting Ratings from xpath\n",
    "        prod_dict['Rating'].append(rating.get_attribute(\"title\"))\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Rating'].append('-')\n",
    "    \n",
    "    try:\n",
    "        n_rating = driver.find_element_by_xpath('//a[@id=\"acrCustomerReviewLink\"]/span')     # Extracting no. of Ratings from xpath\n",
    "        prod_dict['No. of ratings'].append(n_rating.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['No. of ratings'].append('-')\n",
    "    \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('//span[@id=\"priceblock_ourprice\"]')            # Extracting Price from xpath\n",
    "        prod_dict['Price'].append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Price'].append('-')\n",
    "    try:                                                                                     # Extracting Return/Exchange policy from xpath\n",
    "        ret = driver.find_element_by_xpath('//div[@data-name=\"RETURNS_POLICY\"]/span/div[2]/a')\n",
    "        prod_dict['Return/Exchange'].append(ret.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Return/Exchange'].append('-')\n",
    "    try:\n",
    "        delivry = driver.find_element_by_xpath('//div[@id=\"ddmDeliveryMessage\"]/b')         # Extracting Expected Delivery from xpath\n",
    "        prod_dict['Expected Delivery'].append(delivry.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Expected Delivery'].append('-')\n",
    "    \n",
    "    try:\n",
    "        avl = driver.find_element_by_xpath('//div[@id=\"availability\"]/span')                # Extracting Availability from xpath\n",
    "        prod_dict['Availability'].append(avl.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Availability'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Other Details from xpath\n",
    "        dtls = driver.find_element_by_xpath('//ul[@class=\"a-unordered-list a-vertical a-spacing-mini\"]')\n",
    "        prod_dict['Other Details'].append('  ||  '.join(dtls.text.split('\\n')))\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Other Details'].append('-')\n",
    "    \n",
    "    prod_dict['URL'].append(url)                                                            # Saving url\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a689a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df = pd.DataFrame.from_dict(prod_dict)\n",
    "prod_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f9acb",
   "metadata": {},
   "source": [
    "# Q3. Write a python program to access the search bar and search button on images.google.com and scrape 100 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening images.google.com\n",
    "driver.get('https://images.google.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd7ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/div/div[2]/input')    # Finding the search bar using it's xpath\n",
    "search_bar.send_keys(\"fruits\")       # Inputing \"banana\" keyword to search rock images\n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/button')    # Finding the xpath of search button\n",
    "search_button.click()        # Clicking the search button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start scrolling to generate more images on the page...\")\n",
    "# 500 time we scroll down by 10000 in order to generate more images on the website\n",
    "for _ in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9010042",
   "metadata": {},
   "outputs": [],
   "source": [
    " images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(\"H:/Flip ROBO/banana/img\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c5431f",
   "metadata": {},
   "source": [
    "# Q4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Display Resolution”, “Processor”, “Processor Cores”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc1582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asking for user input\n",
    "prod = input(\" Enter the name of the mobile phone you want to search : \")\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(3)\n",
    "try:\n",
    "    login_X_button = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]')                      # Button to close login popup\n",
    "    login_X_button.click()\n",
    "except NoSuchElementException : \n",
    "    print(\"No Login page\")\n",
    "search_bar = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')    # Finding the search bar using it's xpath\n",
    "search_bar.clear()               # Clearing the search bar\n",
    "search_bar.send_keys(prod)       # Inputing keyword to search\n",
    "search_button = driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')    # Finding the xpath of search button\n",
    "search_button.click()        # Clicking the search button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching urls of phones coming on 1st page\n",
    "flip_urls = []\n",
    "urls = driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "for url in urls:\n",
    "    flip_urls.append(url.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9fab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flip_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cda9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_dict = {}\n",
    "flip_dict[\"Brand\"] = []\n",
    "flip_dict[\"Smartphone\"] = []\n",
    "flip_dict[\"Colour\"] = []\n",
    "flip_dict[\"RAM\"] = []\n",
    "flip_dict[\"Storage(ROM)\"] = []\n",
    "flip_dict[\"Primary Camera\"] = []\n",
    "flip_dict[\"Secondary Camera\"] = []\n",
    "flip_dict[\"Display Size\"] = []\n",
    "flip_dict[\"Display Resolution\"] = []\n",
    "flip_dict[\"Processor\"] = []\n",
    "flip_dict[\"Processor Cores\"] = []\n",
    "flip_dict[\"Battery Capacity\"] = []\n",
    "flip_dict[\"Battery Type\"] = []\n",
    "flip_dict[\"Price\"] = []\n",
    "flip_dict[\"URL\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b47cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data from each url\n",
    "for url in flip_urls:\n",
    "    driver.get(url)    # Saving url                                                     \n",
    "    print(\"Scraping URL = \", url)\n",
    "    flip_dict['URL'].append(url)                                                          # Loading the webpage by url\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        read_more = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')     # Button for expanding the specs\n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception Occured. Moving to next page\")\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')      # Extracting Brand from xpath\n",
    "        flip_dict[\"Brand\"].append(brand.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Brand'].append('-')\n",
    "        \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')      # Extracting Brand from xpath\n",
    "        flip_dict['Price'].append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Price'].append('-')\n",
    "        \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')      # Extracting Brand from xpath\n",
    "        flip_dict['Smartphone'].append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Smartphone'].append('-')\n",
    "    \n",
    "    try:\n",
    "        color = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')      # Extracting Name from xpath\n",
    "        flip_dict['Colour'].append(color.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Colour'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_size = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[1]/td[2]/ul/li')  # Extracting Ratings from xpath\n",
    "        flip_dict['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Size'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_res = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[2]/td[2]/ul/li')     # Extracting no. of Ratings from xpath\n",
    "        flip_dict['Display Resolution'].append(disp_res.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Resolution'].append('-')\n",
    "    \n",
    "    try:\n",
    "        pro_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "        if pro_chk.text != \"Processor Type\" : raise NoSuchElementException\n",
    "        processor = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')   # Extracting Price from xpath\n",
    "        flip_dict['Processor'].append(processor.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Processor'].append('-')\n",
    "    \n",
    "    try:                                                                                     # Extracting Return/Exchange policy from xpath\n",
    "        core_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[1]')\n",
    "        if core_chk.text != \"Processor Core\" :\n",
    "            core_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "            if core_chk.text != \"Processor Core\" : \n",
    "                raise NoSuchElementException\n",
    "            else :\n",
    "                cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        else :\n",
    "            cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[2]/ul/li')\n",
    "        flip_dict['Processor Cores'].append(cores.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Processor Cores'].append('-')\n",
    "    \n",
    "    try:\n",
    "        rom = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[1]/td[2]/ul/li')         # Extracting Expected Delivery from xpath\n",
    "        flip_dict['Storage(ROM)'].append(rom.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Storage(ROM)'].append('-')\n",
    "    \n",
    "    try:\n",
    "        ram = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['RAM'].append(ram.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['RAM'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Other Details from xpath\n",
    "        pri_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        flip_dict['Primary Camera'].append(pri_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Primary Camera'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Other Details from xpath\n",
    "        cam_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[1]')\n",
    "        if cam_chk != \"Secondary Camera\" : \n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[1]').text == \"Secondary Camera\":\n",
    "                sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[2]/ul/li')\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[2]/ul/li')\n",
    "        flip_dict['Secondary Camera'].append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Secondary Camera'].append('-')\n",
    "        \n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[1]')\n",
    "            if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['Battery Capacity'].append(bat_cap.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Capacity'].append('-')\n",
    "    \n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[1]')\n",
    "            if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "            bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['Battery Type'].append(bat_typ.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Type'].append('-')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefb75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(flip_dict[\"Brand\"]), len(flip_dict[\"Smartphone\"]), len(flip_dict[\"Processor\"]), len(flip_dict[\"Price\"]), len(flip_dict['URL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_df = pd.DataFrame.from_dict(flip_dict)\n",
    "flip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcefe2b",
   "metadata": {},
   "source": [
    "# Q5. 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening google maps\n",
    "driver.get(\"https://www.google.co.in/maps\")\n",
    "time.sleep(3)\n",
    "\n",
    "city = input('Enter City Name : ')                                         # Enter city to be searched\n",
    "search = driver.find_element_by_id(\"searchboxinput\")                       # locating search bar\n",
    "search.clear()                                                             # clearing search bar\n",
    "time.sleep(2)\n",
    "search.send_keys(city)                                                     # entering values in search bar\n",
    "button = driver.find_element_by_id(\"searchbox-searchbutton\")               # locating search button\n",
    "button.click()                                                             # clicking search button\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead02af9",
   "metadata": {},
   "source": [
    "# Q6. Write a program to scrap details of all the funding deals for second quarter (i.e. July 20 – September 20) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd68204",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://trak.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "button = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_dict = {}\n",
    "fund_dict['Date'] = []\n",
    "fund_dict['Startup Name'] = []\n",
    "fund_dict['Industry/Vertical'] = []\n",
    "fund_dict['Sub-Vertical'] = []\n",
    "fund_dict['Location'] = []\n",
    "fund_dict['Investor'] = []\n",
    "fund_dict['Investment Type'] = []\n",
    "fund_dict['Amount(in USD)'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db221eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(48,51):\n",
    "    driver.find_element_by_xpath('//div[@id=\"tablepress-{}_wrapper\"]/div/label/select/option[4]'.format(i)).click()\n",
    "\n",
    "    # Date\n",
    "    dt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "    for d in dt:\n",
    "        fund_dict['Date'].append(d.text)\n",
    "\n",
    "    # Startup Name\n",
    "    sn = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "    for n in sn:\n",
    "        fund_dict['Startup Name'].append(n.text)\n",
    "    \n",
    "    # Industry/Vertical\n",
    "    ind = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "    for n in ind:\n",
    "        fund_dict['Industry/Vertical'].append(n.text)\n",
    "    \n",
    "    # Sub-Vertical\n",
    "    sv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "    for s in sv:\n",
    "        fund_dict['Sub-Vertical'].append(s.text)\n",
    "\n",
    "    # Location\n",
    "    loc = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "    for l in loc:\n",
    "        fund_dict['Location'].append(l.text)\n",
    "    \n",
    "    # Investor\n",
    "    inv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "    for n in inv:\n",
    "        fund_dict['Investor'].append(n.text)\n",
    "    \n",
    "    # Investment Type\n",
    "    invt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "    for n in invt:\n",
    "        fund_dict['Investment Type'].append(n.text)\n",
    "    \n",
    "    # Amount\n",
    "    amt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "    for a in amt:\n",
    "        fund_dict['Amount(in USD)'].append(a.text)\n",
    "    \n",
    "fund_df = pd.DataFrame(fund_dict)\n",
    "fund_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df852f4f",
   "metadata": {},
   "source": [
    "# Q7. Write a program to scrap all the available details of top 10 gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfde591",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.digit.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clickng on top 10 option \n",
    "top_10=driver.find_element_by_xpath(\"//div[@class='menu']/ul/li[3]/a\")\n",
    "top_10.click()\n",
    "\n",
    "#clicking on laptops option\n",
    "laptops=driver.find_element_by_xpath(\"//ul[@id='top10list']/li[2]\")\n",
    "laptops.click()\n",
    "\n",
    "#best gaming laptops link\n",
    "best_gaming=driver.find_element_by_xpath(\"//div[@id='laptops']/a[3]\")\n",
    "driver.get(best_gaming.get_attribute('href'))\n",
    "\n",
    "#intialising lists\n",
    "name = []\n",
    "price = []\n",
    "OS = []\n",
    "display = []\n",
    "processor = []\n",
    "HDD = []\n",
    "RAM = []\n",
    "weight = []\n",
    "dimension = []\n",
    "GPU = []\n",
    "\n",
    "#names\n",
    "names=driver.find_elements_by_xpath(\"//div[@class='right-container']/div/a/h3\")\n",
    "for i in names:\n",
    "    name.append(i.text)\n",
    "    \n",
    "#os\n",
    "os=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "for i in os:\n",
    "    OS.append(i.text)\n",
    "    \n",
    "#display\n",
    "displays=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "for i in displays:\n",
    "    display.append(i.text)\n",
    "    \n",
    "#processor\n",
    "processors=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[3]/div/div\")\n",
    "for i in processors:\n",
    "    processor.append(i.text)\n",
    "processor\n",
    "\n",
    "#memory\n",
    "memories=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[1]\")#list of specificaion name\n",
    "memories_spec=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")#values of specifiations \n",
    "for i in range(len(memories)):\n",
    "        if memories[i].text=='Memory':\n",
    "            HDD.append(memories_spec[i].text.split('/')[0])\n",
    "            RAM.append(memories_spec[i].text.split('/')[1])\n",
    "        else:\n",
    "            HDD.append('No details available')#append no details as value for memory is missing in some of the laptops\n",
    "            RAM.append('No details available')#append no details as value for memory is missing in some of the laptops\n",
    "\n",
    "#weight\n",
    "weights=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[1]\")#list of specificaion name\n",
    "weight_spec=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[3]\")#values of specifiations\n",
    "for i in range(len(weights)):\n",
    "        if weights[i].text=='Weight':\n",
    "            weight.append(weight_spec[i].text)\n",
    "        \n",
    "#dimension\n",
    "dimension=[]\n",
    "dims=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[1]\")#list of specificaion name\n",
    "dims_spec=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[3]\")#values of specifiations\n",
    "for i in range(len(dims)):\n",
    "        if dims[i].text=='Dimension':\n",
    "            dimension.append(dims_spec[i].text)\n",
    "\n",
    "#graphical processor\n",
    "GPUs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[1]\")#list of specificaion name\n",
    "GPUs_spec=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[3]\")#values of specifiations\n",
    "for i in range(len(GPUs)):\n",
    "        if GPUs[i].text=='Graphics Processor':\n",
    "            GPU.append(GPUs_spec[i].text)\n",
    "        \n",
    "full_specs=[]\n",
    "urls=driver.find_elements_by_xpath(\"//div[@class='full-specs']/span\")#getting the url of full specs links\n",
    "for i in urls:\n",
    "    if i.get_attribute('data-href'):\n",
    "        full_specs.append(i.get_attribute('data-href'))\n",
    "    \n",
    "for i in full_specs:#iterating throug every laptops full specs' page\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        prices=driver.find_element_by_xpath(\"//div[@class='Block-price']/b\")\n",
    "        price.append(prices.text)\n",
    "    except NoSuchElementException:#exception handling for no price details\n",
    "        price.append(\"No details available\")\n",
    "        \n",
    "df=pd.DataFrame({\"Name\":name,\n",
    "                \"Price\":price,\n",
    "                \"OS\":OS,\n",
    "                \"Display\":display,\n",
    "                \"HDD\":HDD,\n",
    "                 \"RAM\":RAM,\n",
    "                \"processor\":processor,\n",
    "                \"weight\":weight,\n",
    "                \"Dimension\":dimension,\n",
    "                \"Graphical processor\":GPU})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49567d5",
   "metadata": {},
   "source": [
    "# 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512205f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "#get the specified url\n",
    "url = \"https://www.forbes.com/?sh=69e6b8c92254\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "#clicking the explore button\n",
    "button = driver.find_element_by_xpath(\"//button[@class='icon--hamburger']\")\n",
    "button.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#select billionaire  \n",
    "bill = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "bill.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#select world billionaire  \n",
    "world_bill= driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "world_bill.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping required data from the web page\n",
    "#Creating empty lists\n",
    "Rank = []\n",
    "Person_Name = []\n",
    "total_net_worth = []\n",
    "Age = []\n",
    "citizenship = []\n",
    "Source = []\n",
    "industry = []\n",
    "\n",
    "\n",
    "while(True):\n",
    "    #Scraping the data of rank\n",
    "    rank_tags= driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "    for rank in rank_tags:\n",
    "        Rank.append(rank.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping the data of names\n",
    "    name_tags= driver.find_elements_by_xpath(\"//div[@class='personName']//div\")\n",
    "    for name in name_tags:\n",
    "        Person_Name.append(name.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of age\n",
    "    age_tags= driver.find_elements_by_xpath(\"//div[@class='age']//div\")\n",
    "    for age in age_tags:\n",
    "        Age.append(age.text)   \n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of citizenship\n",
    "    cit_tags= driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "    for cit in cit_tags:\n",
    "        citizenship.append(cit.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of source of income\n",
    "    sour_tags= driver.find_elements_by_xpath(\"//div[@class='source']\")\n",
    "    for sour in sour_tags:\n",
    "        Source.append(sour.text)    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of Industry\n",
    "    ind_tags= driver.find_elements_by_xpath(\"//div[@class='category']//div\")\n",
    "    for ind in ind_tags:\n",
    "        industry.append(ind.text)\n",
    "        \n",
    "        \n",
    "    #scraping data of net_worth of billionaire \n",
    "    net_tags= driver.find_elements_by_xpath(\"//div[@class='netWorth']//div[1]\")\n",
    "    for net in net_tags:\n",
    "        total_net_worth.append(net.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Clicking on next button\n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath(\"//button[@class='pagination-btn pagination-btn--next ']\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "#Scraping data of net worth\n",
    "Net_Worth = []\n",
    "for i in range(0,len(total_net_worth),2):\n",
    "    Net_Worth.append(total_net_worth[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91277474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Framing data\n",
    "Billionaires=pd.DataFrame({})\n",
    "Billionaires['Rank'] = Rank\n",
    "Billionaires['Names'] = Person_Name\n",
    "Billionaires['Net Worth'] = Net_Worth\n",
    "Billionaires['Age'] = Age\n",
    "Billionaires['Citizenship'] = citizenship\n",
    "Billionaires['Source'] = Source\n",
    "Billionaires['Industry'] = industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing dataframe\n",
    "Billionaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efe647",
   "metadata": {},
   "source": [
    "# 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe7efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae307b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening youtube\n",
    "url = \"https://www.youtube.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecacc31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding element for search bar\n",
    "search_bar = driver.find_element_by_id('search')\n",
    "search_bar.send_keys(\"GOT\")  #entering Video name\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "search_btn = driver.find_element_by_id(\"search-icon-legacy\")  \n",
    "search_btn.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on first video\n",
    "link_click = driver.find_element_by_xpath(\"//yt-formatted-string[@class ='style-scope ytd-video-renderer']\")\n",
    "link_click.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b473e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 time we scroll down to generate more Comments\n",
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make empty lists\n",
    "comments = []\n",
    "comment_time = []\n",
    "Time = []\n",
    "Likes = []\n",
    "No_of_Likes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52daf56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the data of comments\n",
    "cm_tags = driver.find_elements_by_id(\"content-text\")\n",
    "for cm in cm_tags:\n",
    "    if cm.text is None:\n",
    "        comments.append(\"--\")\n",
    "    else:\n",
    "        comments.append(cm.text)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the data of time when comment was posted\n",
    "tm_tags = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "for tm in tm_tags:\n",
    "    Time.append(tm.text)\n",
    "\n",
    "for i in range(0,len(Time),2):\n",
    "    comment_time.append(Time[i])\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaada53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the data of comment likes\n",
    "like_tags = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for like in like_tags:\n",
    "    Likes.append(like.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80036f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe\n",
    "Youtube=pd.DataFrame({})\n",
    "Youtube['Comments'] = comments\n",
    "Youtube['Comment_time'] = comment_time\n",
    "Youtube['Comment upvotes'] = No_of_Likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing dataframe\n",
    "Youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f837f6",
   "metadata": {},
   "source": [
    "# 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "\n",
    "#get the web page with given url\n",
    "url = \"https://www.hostelworld.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the location search bar\n",
    "search_loc = driver.find_element_by_id('search-input-field')\n",
    "# write Lonodn in search bar\n",
    "search_loc.send_keys(\"London\")\n",
    "time.sleep(5)\n",
    "\n",
    "#select london\n",
    "london = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/div[1]/div/div[2]/div[4]/div/div[2]/div/div[1]/div/div/ul/li[2]/div')\n",
    "london.click()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# do click on search button\n",
    "search_btn = driver.find_element_by_id('search-button')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description =[]\n",
    "product_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d09fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the requered informations\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class = 'pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(4)\n",
    "    #fetching hostel name\n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "    #fetching distance from city centre\n",
    "    \n",
    "    try:\n",
    "        dist = driver.find_elements_by_xpath(\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in dist:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "    #fetch privates from price\n",
    "        try:\n",
    "            pvt_price = driver.find_element_by_xpath(\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "    #fetching dorms from price\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]//div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "            #fetching facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements_by_xpath(\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements_by_xpath(\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text )\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "    #lets fetch url of each hostel\n",
    "    p_url = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    #lets click on show more button for description\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    #fetching ratings\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "    #fetching total reviews\n",
    "        \n",
    "    try:\n",
    "        rws = driver.find_element_by_xpath(\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "        #fetch overall review\n",
    "    try:\n",
    "        overall_rw = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall_rw.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "    #fetch property description \n",
    "    try:\n",
    "        disc = driver.find_element_by_xpath(\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e985420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe\n",
    "dff = pd.DataFrame({})\n",
    "dff['Hostel_Name'] = hostel_name\n",
    "dff['Distance fron city centre'] = distance\n",
    "dff['Ratings'] = rating\n",
    "dff['Total_reviews'] = reviews\n",
    "dff['Overall Reviews'] = over_all\n",
    "dff['Privates from price'] = pvt_prices\n",
    "dff['Dorms from price'] = dorms_price\n",
    "dff['Facilities'] = facilities[:83]\n",
    "dff['Description'] = description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db257adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
